{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b583b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fracture/anaconda3/envs/LMCompress+/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Iterator\n",
    "from arithmetic_coder import arithmetic_coder, ac_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self):\n",
    "        self.total_length = 0\n",
    "        self.compressed_length = 0\n",
    "\n",
    "    def compute_ratio(self):\n",
    "        if self.total_length != 0 and self.compressed_length != 0:\n",
    "            return (\n",
    "                self.total_length / self.compressed_length,\n",
    "                self.compressed_length / self.total_length,\n",
    "            )\n",
    "        else:\n",
    "            return 0, 0\n",
    "\n",
    "    def accumulate(self, compressed, original):\n",
    "        if isinstance(compressed, list):\n",
    "            self.compressed_length += len(compressed)\n",
    "        elif isinstance(compressed, int):\n",
    "            self.compressed_length += compressed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported compressed length type: {type(compressed)}\")\n",
    "\n",
    "        if isinstance(original, list):\n",
    "            self.total_length += len(original)\n",
    "        elif isinstance(original, int):\n",
    "            self.total_length += original\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported original length type: {type(original)}\")\n",
    "\n",
    "\n",
    "def compress(compress_input, logits, metric):\n",
    "    \"\"\"\n",
    "    :param compress_input: symbols to be compressed\n",
    "    :param logits: generation probabilities from the model\n",
    "    :param metric: compression metrics\n",
    "    :return: compressed result, a floating number\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # Initialize a Encoder Object\n",
    "    # Precision is for the encoder, not the model\n",
    "    # You must have the same precision for encoder and decoder\n",
    "    # Tricky things here: Though theoratically prefill == decode, but in practice there are numerical problems\n",
    "    encoder = arithmetic_coder.Encoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        output_fn=output.append,\n",
    "    )\n",
    "    # the first symbol should be saved for generation in decoding\n",
    "    start_symbol = compress_input[:, :1]\n",
    "    probs = logits.softmax(dim=-1).to(torch.float32)\n",
    "    pd = torch.gather(probs, dim=-1, index=compress_input[:, 1:].unsqueeze(-1)).squeeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    probs = np.vstack(probs.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    sequence_array = compress_input[:, 1:].detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    pd = pd.squeeze()\n",
    "\n",
    "    # compress the sequence\n",
    "    for symbol, prob, pd_prob in zip(sequence_array, probs, pd):\n",
    "        encoder.encode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob, np.float32), symbol\n",
    "        )\n",
    "    encoder.terminate()\n",
    "\n",
    "    # to visualize and compute metrics, map to str\n",
    "    compressed_bits = \"\".join(map(str, output))\n",
    "    # you can only save in bytes, so need to pad some bits\n",
    "    compressed_bytes, num_padded_bits = ac_utils.bits_to_bytes(compressed_bits)\n",
    "    metric.accumulate(len(compressed_bytes), len(sequence_array))\n",
    "\n",
    "    compress_rate, compress_ratio = metric.compute_ratio()\n",
    "    logger.info(f\"compressed length: {metric.compressed_length}\")\n",
    "    logger.info(f\"original length: {metric.total_length}\")\n",
    "    logger.info(f\"compression ratio: {compress_ratio:.6f}\")\n",
    "    logger.info(f\"compression rate: {compress_rate:.6f}\")\n",
    "\n",
    "    return compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs\n",
    "\n",
    "\n",
    "\n",
    "def decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    model,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_seq_len,\n",
    "    original_sequence=None,\n",
    "    pd=None,\n",
    "    probs=None,\n",
    "    do_test=True,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param compressed_bytes:  compressed data\n",
    "    :param num_padded_bits:  padded bits\n",
    "    :param model: same model as encoder\n",
    "    :param start_symbol: first symbol to generate\n",
    "    :param original_sequence: original symbol sequence, for testing purpose\n",
    "    :param pd: actually not needed, used for testing\n",
    "    :param probs:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # convert bytes back to bit stream\n",
    "    data_iter = iter(\n",
    "        ac_utils.bytes_to_bits(compressed_bytes, num_padded_bits=num_padded_bits)\n",
    "    )\n",
    "\n",
    "    # utils function to read bits\n",
    "    def _input_fn(bit_sequence: Iterator[str] = data_iter) -> int | None:\n",
    "        try:\n",
    "            return int(next(bit_sequence))\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "    # initialize a Decoder Object\n",
    "    decoder = arithmetic_coder.Decoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        input_fn=_input_fn,\n",
    "    )\n",
    "\n",
    "    sequence_array_de = start_symbol.squeeze(0).detach().cpu().numpy()\n",
    "    sequence_array_de_input = start_symbol\n",
    "    target_diff_list = []\n",
    "    target_in_top5_list = []\n",
    "\n",
    "    # loop for decompressing\n",
    "    # pad the input to the original length\n",
    "    sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n",
    "    sequence_array_de_input = torch.nn.functional.pad(sequence_array_de_input, (0, original_seq_len-1), value=0)\n",
    "\n",
    "    for i in range(original_seq_len):\n",
    "        # attention_mask = (sequence_array_de_input != 0).long()\n",
    "        with torch.no_grad():\n",
    "            logits = model(sequence_array_de_input, use_cache=False).logits.to(\n",
    "                torch.float32\n",
    "            )\n",
    "        # get generaton probabilities, decode the next token\n",
    "        prob_de = logits.softmax(dim=-1).detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "        de_token = decoder.decode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob_de[i], np.float32)\n",
    "        )\n",
    "        # using the original probs to decode, for testing purpose\n",
    "        # de_token = decoder.decode(ac_utils.normalize_pdf_for_arithmetic_coding(probs[i]))\n",
    "        # append to the generated sequence\n",
    "        sequence_array_de = np.append(sequence_array_de, de_token)\n",
    "\n",
    "        current_len = len(sequence_array_de)\n",
    "        target_len = original_seq_len\n",
    "\n",
    "        if current_len < target_len:\n",
    "            padded = np.pad(\n",
    "                sequence_array_de, (0, (target_len - current_len)), constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            padded = sequence_array_de\n",
    "        sequence_array_de_input = torch.tensor(\n",
    "            padded, dtype=torch.long, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        if do_test:\n",
    "            top_indices_de = prob_de[i].argsort()[-5:][::-1]\n",
    "            top_indices = probs[i].argsort()[-5:][::-1]\n",
    "\n",
    "            # target diff\n",
    "            target_diff = probs[i, original_sequence[i]] - prob_de[i, original_sequence[i]]\n",
    "            target_diff_list.append(target_diff)\n",
    "\n",
    "            # target in top 5\n",
    "            target_in_top5 = original_sequence[i] in top_indices\n",
    "            target_in_top5_list.append(target_in_top5)\n",
    "            print(\n",
    "                f\"idx: {i}, original token: {original_sequence[i]}, decoder token: {de_token}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"diff probs max: {max(abs(probs[i] - prob_de[i]))}, original sum error: {abs(sum(prob_de[i]) - 1.0)}, decoder sum error: {abs(sum(probs[i]) - 1.0)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"original: {top_indices}, target_in_top5: {target_in_top5} decode: {top_indices_de}, \"\n",
    "            )\n",
    "            print(f\"target diff: {target_diff}\")\n",
    "            if original_sequence[i] != de_token:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "        \n",
    "    return sequence_array_de_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_padded_bytes(filename: str, data: bytes, num_padded_bits: int, original_length: int):\n",
    "    \"\"\"\n",
    "    file format:\n",
    "    - first byte: number of padded bit\n",
    "    - second and third byte: original length (usually, llm context will not exceed 65535)\n",
    "    - subsequent bytes: actual bytes data\n",
    "\n",
    "    :param filename: output file name\n",
    "    :param data: bytes data to write\n",
    "    :param padding_bits: number of padded bits (must be between 0 and 7)\n",
    "    :param original_length: original length of the uncompressed data (in tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 <= num_padded_bits <= 7:\n",
    "        raise ValueError(\"num_padded_bits must be between 0 and 7.\")\n",
    "\n",
    "    if not 0 <= original_length <= 65535:\n",
    "        raise ValueError(\"original_length must be between 0 and 65535.\")\n",
    "\n",
    "    if not isinstance(data, bytes):\n",
    "        raise TypeError(\"data must be of bytes type.\")\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        padding_byte = num_padded_bits.to_bytes(1, 'big')\n",
    "        f.write(padding_byte)\n",
    "        f.write(original_length.to_bytes(2, 'big'))\n",
    "        f.write(data)\n",
    "\n",
    "def read_padded_bytes(filename: str) -> tuple[bytes, int]:\n",
    "    \"\"\"\n",
    "    Read data and padding bits from a file.\n",
    "\n",
    "    :param filename: The name of the file to read.\n",
    "    :return: A tuple containing (bytes data, number of padded bits).\n",
    "             May raise an error if the file is empty or improperly formatted.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # the first byte indicates the number of padded bits\n",
    "        padding_byte = f.read(1)\n",
    "\n",
    "        # If the file is empty, f.read(1) will return an empty bytes object b''\n",
    "        if not padding_byte:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read padding bits byte.\")\n",
    "\n",
    "        original_length_bytes = f.read(2)\n",
    "        if not original_length_bytes:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read original length bytes.\")\n",
    "    \n",
    "        padding_bits = int.from_bytes(padding_byte, 'big')\n",
    "        original_length = int.from_bytes(original_length_bytes, 'big')\n",
    "\n",
    "        data = f.read()\n",
    "        \n",
    "        return data, padding_bits, original_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a8cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x06\\x07O Z:\\xce~\\xf5\\xd1g\\xfage\\x92\\xc0\\x9dT9=\\xa2\\xa6'\n",
      "4\n",
      "45\n",
      "b'\\x06\\x07O Z:\\xce~\\xf5\\xd1g\\xfage\\x92\\xc0\\x9dT9=\\xa2\\xa6'\n",
      "4\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12394/3359742560.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, original token: 264, decoder token: 264\n",
      "diff probs max: 0.0, original sum error: 0.0003540515899658203, decoder sum error: 0.0003540515899658203\n",
      "original: [  264   279   419   458 21495], target_in_top5: True decode: [  264   279   419   458 21495], \n",
      "target diff: 0.0\n",
      "idx: 1, original token: 33634, decoder token: 33634\n",
      "diff probs max: 0.0, original sum error: 0.0002461075782775879, decoder sum error: 0.0002461075782775879\n",
      "original: [ 6888  3654  2613 24017  8038], target_in_top5: False decode: [ 6888  3654  2613 24017  8038], \n",
      "target diff: 0.0\n",
      "idx: 2, original token: 9271, decoder token: 9271\n",
      "diff probs max: 0.0, original sum error: 0.0003039240837097168, decoder sum error: 0.0003039240837097168\n",
      "original: [ 2484   501  4401  3271 38910], target_in_top5: False decode: [ 2484   501  4401  3271 38910], \n",
      "target diff: 0.0\n",
      "idx: 3, original token: 11, decoder token: 11\n",
      "diff probs max: 0.0, original sum error: 0.00023746490478515625, decoder sum error: 0.00023746490478515625\n",
      "original: [ 11 504 429 369 304], target_in_top5: True decode: [ 11 504 429 369 304], \n",
      "target diff: 0.0\n",
      "idx: 4, original token: 13923, decoder token: 13923\n",
      "diff probs max: 0.0, original sum error: 0.00032460689544677734, decoder sum error: 0.00032460689544677734\n",
      "original: [  264   279 11811 13923   432], target_in_top5: True decode: [  264   279 11811 13923   432], \n",
      "target diff: 0.0\n",
      "idx: 5, original token: 11105, decoder token: 11105\n",
      "diff probs max: 0.0, original sum error: 0.00013810396194458008, decoder sum error: 0.00013810396194458008\n",
      "original: [ 614  518  504 1977  304], target_in_top5: False decode: [ 614  518  504 1977  304], \n",
      "target diff: 0.0\n",
      "idx: 6, original token: 264, decoder token: 264\n",
      "diff probs max: 0.0, original sum error: 0.0002269148826599121, decoder sum error: 0.0002269148826599121\n",
      "original: [ 429  264  279  458 5904], target_in_top5: True decode: [ 429  264  279  458 5904], \n",
      "target diff: 0.0\n",
      "idx: 7, original token: 58113, decoder token: 58113\n",
      "diff probs max: 0.0, original sum error: 0.0002815723419189453, decoder sum error: 0.0002815723419189453\n",
      "original: [  501  8597   220 16770 14862], target_in_top5: False decode: [  501  8597   220 16770 14862], \n",
      "target diff: 0.0\n",
      "idx: 8, original token: 315, decoder token: 315\n",
      "diff probs max: 0.0, original sum error: 0.0001525282859802246, decoder sum error: 0.0001525282859802246\n",
      "original: [ 315 3563  429  220 2832], target_in_top5: True decode: [ 315 3563  429  220 2832], \n",
      "target diff: 0.0\n",
      "idx: 9, original token: 87549, decoder token: 87549\n",
      "diff probs max: 0.0, original sum error: 0.00027889013290405273, decoder sum error: 0.00027889013290405273\n",
      "original: [  220 60766 14538 50933  8380], target_in_top5: False decode: [  220 60766 14538 50933  8380], \n",
      "target diff: 0.0\n",
      "idx: 10, original token: 43029, decoder token: 43029\n",
      "diff probs max: 0.0, original sum error: 3.2782554626464844e-05, decoder sum error: 3.2782554626464844e-05\n",
      "original: [53130 43029  6436  1361  1862], target_in_top5: True decode: [53130 43029  6436  1361  1862], \n",
      "target diff: 0.0\n",
      "idx: 11, original token: 5382, decoder token: 5382\n",
      "diff probs max: 0.0, original sum error: 0.00023674964904785156, decoder sum error: 0.00023674964904785156\n",
      "original: [ 304 5382  389  429   11], target_in_top5: True decode: [ 304 5382  389  429   11], \n",
      "target diff: 0.0\n",
      "idx: 12, original token: 304, decoder token: 304\n",
      "diff probs max: 0.0, original sum error: 0.0001348257064819336, decoder sum error: 0.0001348257064819336\n",
      "original: [ 304  389 4766 4221 2878], target_in_top5: True decode: [ 304  389 4766 4221 2878], \n",
      "target diff: 0.0\n",
      "idx: 13, original token: 264, decoder token: 264\n",
      "diff probs max: 0.0, original sum error: 0.0002696514129638672, decoder sum error: 0.0002696514129638672\n",
      "original: [  279   264 71687   458  3550], target_in_top5: True decode: [  279   264 71687   458  3550], \n",
      "target diff: 0.0\n",
      "idx: 14, original token: 8699, decoder token: 8699\n",
      "diff probs max: 0.0, original sum error: 0.00028568506240844727, decoder sum error: 0.00028568506240844727\n",
      "original: [ 8699 25385 23603 12767  2613], target_in_top5: True decode: [ 8699 25385 23603 12767  2613], \n",
      "target diff: 0.0\n",
      "idx: 15, original token: 11, decoder token: 11\n",
      "diff probs max: 0.0, original sum error: 0.0002511143684387207, decoder sum error: 0.0002511143684387207\n",
      "original: [ 3082   949    11 16301  5537], target_in_top5: True decode: [ 3082   949    11 16301  5537], \n",
      "target diff: 0.0\n",
      "idx: 16, original token: 8597, decoder token: 8597\n",
      "diff probs max: 0.0, original sum error: 0.00020927190780639648, decoder sum error: 0.00020927190780639648\n",
      "original: [24203 70013 93988 23603   939], target_in_top5: False decode: [24203 70013 93988 23603   939], \n",
      "target diff: 0.0\n",
      "idx: 17, original token: 650, decoder token: 650\n",
      "diff probs max: 0.0, original sum error: 0.0001252293586730957, decoder sum error: 0.0001252293586730957\n",
      "original: [  650  9788 70013 67522 13065], target_in_top5: True decode: [  650  9788 70013 67522 13065], \n",
      "target diff: 0.0\n",
      "idx: 18, original token: 68231, decoder token: 68231\n",
      "diff probs max: 0.0, original sum error: 5.841255187988281e-05, decoder sum error: 5.841255187988281e-05\n",
      "original: [68231 90818 28868 40356  8548], target_in_top5: True decode: [68231 90818 28868 40356  8548], \n",
      "target diff: 0.0\n",
      "idx: 19, original token: 1151, decoder token: 1151\n",
      "diff probs max: 0.0, original sum error: 4.291534423828125e-06, decoder sum error: 4.291534423828125e-06\n",
      "original: [ 1151  1608  2721 15368  2066], target_in_top5: True decode: [ 1151  1608  2721 15368  2066], \n",
      "target diff: 0.0\n",
      "idx: 20, original token: 33581, decoder token: 33581\n",
      "diff probs max: 0.0, original sum error: 0.0002155303955078125, decoder sum error: 0.0002155303955078125\n",
      "original: [ 3082  5537   949 23603    11], target_in_top5: False decode: [ 3082  5537   949 23603    11], \n",
      "target diff: 0.0\n",
      "idx: 21, original token: 11, decoder token: 11\n",
      "diff probs max: 0.0, original sum error: 0.0001417398452758789, decoder sum error: 0.0001417398452758789\n",
      "original: [ 304  315  389   13 3143], target_in_top5: False decode: [ 304  315  389   13 3143], \n",
      "target diff: 0.0\n",
      "idx: 22, original token: 304, decoder token: 304\n",
      "diff probs max: 0.0, original sum error: 0.0002617835998535156, decoder sum error: 0.0002617835998535156\n",
      "original: [304 323 892 220 264], target_in_top5: True decode: [304 323 892 220 264], \n",
      "target diff: 0.0\n",
      "idx: 23, original token: 279, decoder token: 279\n",
      "diff probs max: 0.0, original sum error: 0.00029158592224121094, decoder sum error: 0.00029158592224121094\n",
      "original: [  279   264  1128 90750  4882], target_in_top5: True decode: [  279   264  1128 90750  4882], \n",
      "target diff: 0.0\n",
      "idx: 24, original token: 1597, decoder token: 1597\n",
      "diff probs max: 0.0, original sum error: 0.0003211498260498047, decoder sum error: 0.0003211498260498047\n",
      "original: [ 1597 75338 11888 23501 18070], target_in_top5: True decode: [ 1597 75338 11888 23501 18070], \n",
      "target diff: 0.0\n",
      "idx: 25, original token: 288, decoder token: 288\n",
      "diff probs max: 0.0, original sum error: 4.500150680541992e-05, decoder sum error: 4.500150680541992e-05\n",
      "original: [  288  5307 12715   441 86127], target_in_top5: True decode: [  288  5307 12715   441 86127], \n",
      "target diff: 0.0\n",
      "idx: 26, original token: 40014, decoder token: 40014\n",
      "diff probs max: 0.0, original sum error: 8.636713027954102e-05, decoder sum error: 8.636713027954102e-05\n",
      "original: [40014 23501   315 16301    13], target_in_top5: True decode: [40014 23501   315 16301    13], \n",
      "target diff: 0.0\n",
      "idx: 27, original token: 13, decoder token: 13\n",
      "diff probs max: 0.0, original sum error: 0.00012612342834472656, decoder sum error: 0.00012612342834472656\n",
      "original: [315  13  11 304 624], target_in_top5: True decode: [315  13  11 304 624], \n",
      "target diff: 0.0\n",
      "idx: 28, original token: 7418, decoder token: 7418\n",
      "diff probs max: 0.0, original sum error: 0.00021821260452270508, decoder sum error: 0.00021821260452270508\n",
      "original: [ 576 2379 4220 1096  758], target_in_top5: False decode: [ 576 2379 4220 1096  758], \n",
      "target diff: 0.0\n",
      "idx: 29, original token: 803, decoder token: 803\n",
      "diff probs max: 0.0, original sum error: 0.00023359060287475586, decoder sum error: 0.00023359060287475586\n",
      "original: [3498  803  279 1573  304], target_in_top5: True decode: [3498  803  279 1573  304], \n",
      "target diff: 0.0\n",
      "idx: 30, original token: 14861, decoder token: 14861\n",
      "diff probs max: 0.0, original sum error: 0.00015664100646972656, decoder sum error: 0.00015664100646972656\n",
      "original: [33634 14861 48913 67734  7897], target_in_top5: True decode: [33634 14861 48913 67734  7897], \n",
      "target diff: 0.0\n",
      "idx: 31, original token: 311, decoder token: 311\n",
      "diff probs max: 0.0, original sum error: 9.97781753540039e-05, decoder sum error: 9.97781753540039e-05\n",
      "original: [ 11 374 572 311  25], target_in_top5: True decode: [ 11 374 572 311  25], \n",
      "target diff: 0.0\n",
      "idx: 32, original token: 279, decoder token: 279\n",
      "diff probs max: 0.0, original sum error: 0.00018131732940673828, decoder sum error: 0.00018131732940673828\n",
      "original: [13923   279  1657  1105 11811], target_in_top5: True decode: [13923   279  1657  1105 11811], \n",
      "target diff: 0.0\n",
      "idx: 33, original token: 11811, decoder token: 11811\n",
      "diff probs max: 0.0, original sum error: 0.00018233060836791992, decoder sum error: 0.00018233060836791992\n",
      "original: [11811 13923 12344  2083   584], target_in_top5: True decode: [11811 13923 12344  2083   584], \n",
      "target diff: 0.0\n",
      "idx: 34, original token: 572, decoder token: 572\n",
      "diff probs max: 0.0, original sum error: 0.0001227259635925293, decoder sum error: 0.0001227259635925293\n",
      "original: [ 572   11  374  879 1033], target_in_top5: True decode: [ 572   11  374  879 1033], \n",
      "target diff: 0.0\n",
      "idx: 35, original token: 279, decoder token: 279\n",
      "diff probs max: 0.0, original sum error: 0.00013071298599243164, decoder sum error: 0.00013071298599243164\n",
      "original: [ 429  279 1246  862  264], target_in_top5: True decode: [ 429  279 1246  862  264], \n",
      "target diff: 0.0\n",
      "idx: 36, original token: 2097, decoder token: 2097\n",
      "diff probs max: 0.0, original sum error: 0.0001977086067199707, decoder sum error: 0.0001977086067199707\n",
      "original: [ 2097 18335  9362  9271  1379], target_in_top5: True decode: [ 2097 18335  9362  9271  1379], \n",
      "target diff: 0.0\n",
      "idx: 37, original token: 429, decoder token: 429\n",
      "diff probs max: 0.0, original sum error: 0.00011837482452392578, decoder sum error: 0.00011837482452392578\n",
      "original: [ 429  807  279 1493  419], target_in_top5: True decode: [ 429  807  279 1493  419], \n",
      "target diff: 0.0\n",
      "idx: 38, original token: 279, decoder token: 279\n",
      "diff probs max: 0.0, original sum error: 0.00015664100646972656, decoder sum error: 0.00015664100646972656\n",
      "original: [ 279 1493  807  419  678], target_in_top5: True decode: [ 279 1493  807  419  678], \n",
      "target diff: 0.0\n",
      "idx: 39, original token: 87549, decoder token: 87549\n",
      "diff probs max: 0.0, original sum error: 0.00014352798461914062, decoder sum error: 0.00014352798461914062\n",
      "original: [87549 19970  9898 58113 81830], target_in_top5: True decode: [87549 19970  9898 58113 81830], \n",
      "target diff: 0.0\n",
      "idx: 40, original token: 43029, decoder token: 43029\n",
      "diff probs max: 0.0, original sum error: 4.5299530029296875e-06, decoder sum error: 4.5299530029296875e-06\n",
      "original: [43029  1512  4241 53130   651], target_in_top5: True decode: [43029  1512  4241 53130   651], \n",
      "target diff: 0.0\n",
      "idx: 41, original token: 12290, decoder token: 12290\n",
      "diff probs max: 0.0, original sum error: 0.00017261505126953125, decoder sum error: 0.00017261505126953125\n",
      "original: [ 1033   525  1030    11 12163], target_in_top5: False decode: [ 1033   525  1030    11 12163], \n",
      "target diff: 0.0\n",
      "idx: 42, original token: 4727, decoder token: 4727\n",
      "diff probs max: 0.0, original sum error: 0.00022393465042114258, decoder sum error: 0.00022393465042114258\n",
      "original: [  264   279 15154  3738   323], target_in_top5: False decode: [  264   279 15154  3738   323], \n",
      "target diff: 0.0\n",
      "idx: 43, original token: 6364, decoder token: 6364\n",
      "diff probs max: 0.0, original sum error: 0.00020170211791992188, decoder sum error: 0.00020170211791992188\n",
      "original: [ 6364 15154  3738    11 19458], target_in_top5: True decode: [ 6364 15154  3738    11 19458], \n",
      "target diff: 0.0\n",
      "idx: 44, original token: 13, decoder token: 13\n",
      "diff probs max: 0.0, original sum error: 0.00012636184692382812, decoder sum error: 0.00012636184692382812\n",
      "original: [ 11 624  13 323   0], target_in_top5: True decode: [ 11 624  13 323   0], \n",
      "target diff: 0.0\n",
      "[  641   264 33634  9271    11 13923 11105   264 58113   315 87549 43029\n",
      "  5382   304   264  8699    11  8597   650 68231  1151 33581    11   304\n",
      "   279  1597   288 40014    13  7418   803 14861   311   279 11811   572\n",
      "   279  2097   429   279 87549 43029 12290  4727  6364    13]\n",
      "tensor([[  641,   264, 33634,  9271,    11, 13923, 11105,   264, 58113,   315,\n",
      "         87549, 43029,  5382,   304,   264,  8699,    11,  8597,   650, 68231,\n",
      "          1151, 33581,    11,   304,   279,  1597,   288, 40014,    13,  7418,\n",
      "           803, 14861,   311,   279, 11811,   572,   279,  2097,   429,   279,\n",
      "         87549, 43029, 12290,  4727,  6364,    13]])\n",
      "Compression time: 1.57 seconds\n",
      "Decompression time: 26.79 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# model and tokenizer loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"pretrained/Qwen2.5-0.5B\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pretrained/Qwen2.5-0.5B\", use_fast=False)\n",
    "llm.eval()\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# data\n",
    "# sample_text = \"Super simple text to be tested.\"\n",
    "sample_text = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "# sample_text = r\"\"\"Greenhouse gas emissions from the burning of fossil fuels have pushed the acidity of the world's oceans past a safe threshold, scientists warn, threatening their ability to sustain shellfish and corals and help us in the fight against climate change. A new report says that ocean acidification is the latest \"planetary boundary\" to be crossed, a reference to a set of warning signs related to key planetary systems that keep the Earth safe for human civilization. Other planetary boundaries that have already been crossed — including dangerous levels of chemical pollution, the warming atmosphere and changes to the nutrient cycle — have already signalled threats to people.  \"Go outside of these boundaries and you first enter a danger zone, with higher risk of causing changes that would undermine that ability to support human life and human development,\" said Johan Rockström, director of the Potsdam Institute for Climate Impact Research, which is behind the Planetary Health Check report released on Wednesday. \"And once you are at the upper end of the uncertainty range ... you enter the red zone, the high-risk zone where most science agrees that we are very likely to depress buttons that will cause irreversible changes, basically committing ourselves to drifting away from livable conditions on Earth.\" Adding the oceans to the planetary boundaries list is a major concern because of the billions of people who depend on them. Continuing ocean acidification could not only destroy fisheries that people rely on for food but reduce the ability of the ocean to absorb carbon dioxide and moderate global warming. As humans burn fossil fuels and pump carbon dioxide into the atmosphere, it's estimated that the ocean is absorbing more than a quarter of that CO2.  \"Just like when we add carbon dioxide to Coke or soda, that makes the soft drink more acidic,\" said Christopher Harley, a professor who studies climate change and the ocean at the University of British Columbia.  But when CO2 is absorbed, the chemical process effectively lowers the availability of a mineral that certain marine life — from shellfish to coral — need to develop their bodies. \"It makes it harder to build shells — and you need to add shell if you want to grow bigger,\" Harley explained, comparing it to the construction of a house.  \"All of a sudden, the building materials become more costly. You're either going to build smaller homes or not as many.\" \"\"\"\n",
    "\n",
    "# work flow\n",
    "compression_start_time = time.time()\n",
    "\n",
    "tokenized = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "metric = Metric()\n",
    "with torch.inference_mode():\n",
    "    # we don't need the last token's logits\n",
    "    logits = (\n",
    "        llm(tokenized[\"input_ids\"], use_cache=False).logits[:, :-1].to(torch.float32)\n",
    "    )\n",
    "compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs = compress(\n",
    "    tokenized[\"input_ids\"], logits, metric\n",
    ")\n",
    "\n",
    "compression_end_time = time.time()\n",
    "\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "original_length = tokenized[\"input_ids\"].shape[1] - 1\n",
    "print(original_length)\n",
    "write_padded_bytes(\"compressed.bin\", compressed_bytes, num_padded_bits, original_length)\n",
    "compressed_bytes, num_padded_bits, original_length = read_padded_bytes(\"compressed.bin\")\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "print(original_length)\n",
    "\n",
    "decompression_start_time = time.time()\n",
    "\n",
    "decompressed = decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    llm,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_length,\n",
    "    sequence_array,\n",
    "    pd,\n",
    "    probs,\n",
    "    do_test=True,\n",
    ")\n",
    "\n",
    "decompression_end_time = time.time()\n",
    "\n",
    "print(tokenized[\"input_ids\"].squeeze(0).numpy())\n",
    "print(decompressed)\n",
    "\n",
    "print(f\"Compression time: {compression_end_time - compression_start_time:.2f} seconds\")\n",
    "print(f\"Decompression time: {decompression_end_time - decompression_start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LMCompress+",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
